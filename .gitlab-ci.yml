# The VG tests are going to use toil-vg.
# toil-vg needs to be able to mount paths it can see into Docker containers.
# There's no good way to do that when running Docker containers as siblings of a container containing toil-vg.
# So we either have to genuinely nest Docker inside another Docker, or we have to run the build on the real host.

# Pull in an image that has our apt packages pre-installed, to save time installing them for every test.
# # This one also has Singularity and doesn't start Docker and hang if unpriviliged
image: quay.io/vgteam/vg_ci_prebake:noentrypoint

before_script:
  - whoami
  - sudo apt-get -q -y update
  # Make sure we have some curl stuff for pycurl which we need for some Python stuff
  # We also need Node >4 for junit merging (so we need to be on Ubuntu 18.04+)
  # And the CI report upload needs uuidgen from uuid-runtime
  - sudo apt-get -q -y install docker.io python-pip python-virtualenv libcurl4-gnutls-dev python-dev npm nodejs node-gyp nodejs-dev libssl1.0-dev uuid-runtime
  - which junit-merge || sudo npm install -g junit-merge
  - cat /etc/hosts
  
# We have two pipeline stages: build to make a Docker, and test to run tests.
# TODO: make test stage parallel
stages:
  - build
  - test
  - report
  

# We define one job to do the build
build-job:
  stage: build
  script: 
    # Note that Singularity requires root to actually build a container
    # TODO: doesn't that make it sort of useless?
    # It should be satisfied that it has root inside a Docker container though.
    - sudo bash vgci/vgci.sh -t None -H -S -d vg.sif
  artifacts:
    paths:
      # Send the built vg container around as a file, since we can't push Singularity-built containers to Quay.
      - vg.sif
    expire_in: 1 days
    
# We define a second follow-on phase to run the tests
# Note that WE ONLY RUN TESTS LISTED IN vgci/test-list.txt
test-job:
  stage: test
  # Run in parallel, setting CI_NODE_INDEX and CI_NODE_TOTAL
  # We will find our share of tests from vgci/test-list.txt and run them
  # We ought to run one job per test, but we can wrap around.
  parallel: 19 
  script:
    - mkdir -p junit
    # Make sure IO to Gitlab is in blocking mode so we can't swamp it and crash
    - vgci/blockify.py bash vgci/vgci-parallel-wrapper.sh vgci/test-list.txt vg.sif ${CI_NODE_INDEX} ${CI_NODE_TOTAL} ./junit ./test_output
  artifacts:
    # Let Gitlab see the junit report
    reports:
      junit: junit/*.xml
    paths:
      - junit/*.xml
      - test_output/*
    # Make sure they get artifact'd even if (especially when) the tests fail
    when: always
    expire_in: 3 days

# We have a final job in the last stage to compose an HTML report
report-job:
  stage: report
  # Run this even when the tests fail, because otherwise we won't hear about it.
  when: always
  # All artifacts from previous stages are available
  script:
    # Collect all the junit files from all the test jobs into one
    - junit-merge -o junit.all.xml junit/*.xml
    # All the test output folder artifacts should automatically merge.
    # Make the report and post it.
    # We still need the Docker for version detection.
    # Make sure IO to Gitlab is in blocking mode so we can't swamp it and crash
    - vgci/blockify.py bash vgci/vgci.sh -J junit.all.xml -S -D vg.sif -W test_output
   
  
